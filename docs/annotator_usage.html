---

title: Using the Annotator


keywords: fastai
sidebar: home_sidebar

summary: "Parametrically resample 3D scans into vision datasets."
description: "Parametrically resample 3D scans into vision datasets."
nb_path: "nbs/07_annotator_usage.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/07_annotator_usage.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Omnidata annotater is a tool to parametrically resample 3D scans and output a vision dataset.</p>
<video width="100%" height="464" playsinline="" autoplay="" center="" loop="" muted="" class="video-bg" id="video-bg" poster="./loading.gif">
<source src="https://omnidata.vision/assets/videos/input_output.mp4" type="video/mp4" alt="HTML5 background video">
</video><p>The annotator exposes a simple command-line interface (CLI). To simplify actually using it in practice, we provide a convenient <a href="https://github.com/EPFL-VILAB/omnidata-annotator#installation">Dockerized implementation</a>. The <a href="https://github.com/EPFL-VILAB/omnidata-annotator">source code</a> is available on github.</p>
<h2 id="Demo">Demo<a class="anchor-link" href="#Demo"> </a></h2><p>To run an example (generating a dataset from a single building in the Habitat-Matterport 3D dataset), check out the <a href="https://github.com/EPFL-VILAB/omnidata-annotator#quickstart-run-demo">demo in the main annotator repository</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Handling-annotator-outputs">Handling annotator outputs<a class="anchor-link" href="#Handling-annotator-outputs"> </a></h3><h4 id="Using-the-data-for-training-vision-models">Using the data for training vision models<a class="anchor-link" href="#Using-the-data-for-training-vision-models"> </a></h4><p>Pytorch dataloaders. <a href="/omnidata-annotator/dataloaders.html">Link</a></p>
<h3 id="Distributing-the-data">Distributing the data<a class="anchor-link" href="#Distributing-the-data"> </a></h3><h4 id="Uploading-data-to-a-server">Uploading data to a server<a class="anchor-link" href="#Uploading-data-to-a-server"> </a></h4><p>Compressing &amp; uploading the data with <code>omnitools.upload</code></p>
<h4 id="Downloading-data-from-a-server">Downloading data from a server<a class="anchor-link" href="#Downloading-data-from-a-server"> </a></h4><p>Downloading, validating, and decompressing with <code>omnitools.download</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="More-documentation">More documentation<a class="anchor-link" href="#More-documentation"> </a></h2><p>More documentation is <a href="/omnidata-tools/annotator_documentation.html">available here</a></p>

</div>
</div>
</div>
</div>
 

