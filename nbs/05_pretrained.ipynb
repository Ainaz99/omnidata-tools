{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Models + Demo\n",
    "\n",
    "> This page is for downloading and using the pretrained models in PyTorch. You can also try a <a href=//omnidata.vision/demo>demo in your browser</a> or <a href=//docs.omnidata.vision/training.html>train your own models</a>.\n",
    "Short explanation of models: to demonstrate that data is capable of training strong models and not too much limited by rendering, mesh coarseness, etc, we train some models for different tasks. At the time of publishing, the models were comparable or better than sota (link oasis) for common multiple vision tasks.\n",
    "\n",
    "We provide several of these models here: (maybe add one for curvature or sth?).\n",
    "\n",
    "[ Images of results ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/Ainaz99/omnidata-tools\n",
    "cd omnidata-tools/torch\n",
    "conda create -n testenv -y python=3.8\n",
    "source activate testenv\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "You can see the complete list of required packages in [omnidata-tools/torch/requirements.txt](https://github.com/Ainaz99/omnidata-tools/blob/main/requirements.txt). We recommend using virtualenv for the installation.\n",
    "\n",
    "## Pretrained Models\n",
    "We are providing our pretrained models which (as of publishing time) have state-of-the-art performance in depth and surface normal estimation.\n",
    "\n",
    "#### Network Architecture\n",
    "The surface normal network is based on the [UNet](https://arxiv.org/pdf/1505.04597.pdf) architecture (6 down/6 up). It is trained with both angular and L1 loss and input resolutions between 256 and 512.\n",
    "\n",
    "The depth networks have DPT-based architectures (similar to [MiDaS v3.0](https://github.com/isl-org/MiDaS)) and are trained with scale- and shift-invariant loss and scale-invariant gradient matching term introduced in [MiDaS](https://arxiv.org/pdf/1907.01341v3.pdf), and also [virtual normal loss](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yin_Enforcing_Geometric_Constraints_of_Virtual_Normal_for_Depth_Prediction_ICCV_2019_paper.pdf). You can see a public implementation of the MiDaS loss [here](#midas-implementation). We provide 2 pretrained depth models for both DPT-hybrid and DPT-large architectures with input resolution 384.\n",
    "\n",
    "#### Download pretrained models\n",
    "```bash\n",
    "sh ./tools/download_depth_models.sh\n",
    "sh ./tools/download_surface_normal_models.sh\n",
    "```\n",
    "These will download the pretrained models for `depth` and `normals` to a folder called `./pretrained_models`.\n",
    "\n",
    "## Run our models on your own image\n",
    "After downloading the [pretrained models](#pretrained-models), you can run them on your own image with the following command:\n",
    "```bash\n",
    "python demo.py --task $TASK --img_path $PATH_TO_IMAGE_OR_FOLDER --output_path $PATH_TO_SAVE_OUTPUT\n",
    "```\n",
    "The `--task` flag should be either `normal` or `depth`. To run the script for a `normal` target on an [example image](./assets/demo/test1.png):\n",
    "```bash\n",
    "python demo.py --task normal --img_path assets/demo/test1.png --output_path assets/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  |   |   |   |  |  |  |\n",
    "| :-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n",
    "| <img src=\"/omnidata-tools/images/torch/demo/test1.png\" style='max-width: 100%;'/> |  <img src=\"/omnidata-tools/images/torch/demo/test2.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test3.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test4.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test5.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test6.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test7.png\" style='max-width: 100%;'/> |\n",
    "| <img src=\"/omnidata-tools/images/torch/demo/test1_normal.png\" style='max-width: 100%;'/> |  <img src=\"/omnidata-tools/images/torch/demo/test2_normal.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test3_normal.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test4_normal.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test5_normal.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test6_normal.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test7_normal.png\" style='max-width: 100%;'/> |\n",
    "| <img src=\"/omnidata-tools/images/torch/demo/test1_depth.png\" style='max-width: 100%;'/> |  <img src=\"/omnidata-tools/images/torch/demo/test2_depth.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test3_depth.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test4_depth.png\" style='max-width: 100%;'/>  | <img src=\"/omnidata-tools/images/torch/demo/test5_depth.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test6_depth.png\" style='max-width: 100%;'/> | <img src=\"/omnidata-tools/images/torch/demo/test7_depth.png\" style='max-width: 100%;'/> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "If you find the code or models useful, please cite our paper:\n",
    "```\n",
    "@inproceedings{eftekhar2021omnidata,\n",
    "  title={Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets From 3D Scans},\n",
    "  author={Eftekhar, Ainaz and Sax, Alexander and Malik, Jitendra and Zamir, Amir},\n",
    "  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n",
    "  pages={10786--10796},\n",
    "  year={2021}\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
